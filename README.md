# Sully.ai Interpreter

**Demo**: https://sully.johncronk.com/

1. Load the page.
2. Allow microphone.
3. Say "Sully translate" to start.
4. Say "Sully stop" to stop.

## Features

1. **Hands-free**: The browser speech recognition API is used to support the "Sully" wake word so that visits can be started and completed solely through speech. Say **"Sully translate"** to clear the current conversation and start a new one, say **"Sully stop"** to end it.
2. **Live Metadata for Actions & Language Detection**: Out-of-bound conversation events are used to query the model for the language of each utterance, as well as whether any actions were requested during it. I spent a while trying to get the realtime model to execute live function calls as described in the project spec, but found it too unreliable. It had a high probabilty of missing actions, as well as duplicating them. With more time, I would finish the code I have during metadata collection to trigger the actions more reliably and then de-duplicate them with the actions detected during the summary (that is **if** live actions would even more desired by a customer as opposed to summary actions).
3. **Summary & Action Execution**: 4o is used to generate a summary and list of action items which are executed immediately against the [webhooks API](https://webhook.site/#!/view/7dd3d15d-c150-482e-afc7-f147823f51cb/eb6c8388-36bd-468f-a7e9-be711b6474fd/1).
4. **Persistent & Queryable**: There's a Load Conversation button to let you load any previous conversation.

## Key Decisions

**1. WebRTC vs. WebSockets**:

**WebRTC** for low latency client interaction supporting a live conversation.

**2. Node vs Python**:

**Node**. This was a low stakes decision which minimizes development complexity, keeping the project stack consistent. There were no clear features which depended on having a Python server. I considered whether there were any key NLP library use cases which might be better supported, but found sufficient support with just Node and the existing integration with LLMs (which can handle most NLP tasks).

**3. Database**:

**MongoDB Atlas**. This allows for flexible conversation schema writes and queries, making it easy to identify and query new conversation attributes in the future. MongoDB Atlas can also support vector embeddings for future RAG use cases, which could be appropriate for this app (RAG queries of past patient visits or even up-to-date medical information). Furthermore, MongoDB could be used to support on-premise deployment, which may be necessary for some healthcare customers. Another strong candidate I considered was Azure CosmosDB for its similar features, but decided plain Mongo was better for the potential of on-premise.

**4. Frontend Hosting**

**S3/CloudFront** because it is simple, cheap, and scalable for any SPA.

**5. Backend Hosting**

**Heroku**. This could be scaled and hosted on-premise with a service like ECS/AWS Outpost, but I did not have time for that.

## Next Steps & Issues

- Audio often truncates on the first message.
- Have not found any VAD configuration which reliably handles background noise.
- Need to add checks to better coordinate mapping user utterances, translations, and metadata together. Currently, if they are generated at signficantly different times they can be mis-mapped. I did not think of any 100% reliable way to do this with the realtime events API. This could be improved with heuristics such as noticing blank strings, large time gaps, and common error strings generated by the LLM.
- The realtime model loves to hallucinate names (e.g. "Dr. Cronk" is randomly "Dr. Gomez").
- I didn't realize I had Heroku's automated certificate management on when it shouldn't have been and wasted a lot of time trying to figure out why it kept having a DNS failure during my initial attempt to deploy (whoops).
- Other obvious additions such as:
  - Make the app multi-user.
  - Finish the live action handling.
  - Improve the note structure & action metadata.

## Approach/Tools Used

I used Cursor & Claude 3.5/3.7 for initial project setup and brainstorming. I pulled in the realtime docs locally to make sure Claude stopped hallucinating the API for any changes I asked it to make. The app is simple enough that I didn't have to expand beyond 1 main React component, server API file, and database model. Most of the time was spent planning the interface/architecture and learning/wresting with the realtime API to get what performance I could out of it.
